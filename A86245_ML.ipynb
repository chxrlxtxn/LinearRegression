{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "v3wiiuxgyx3darwxnu22",
   "authorId": "4432515473716",
   "authorName": "31636895",
   "authorEmail": "sri.t.1@bwc.ohio.gov",
   "sessionId": "d8ca7755-6441-4b56-9cc9-92533b4ef904",
   "lastEditTime": 1772042268803
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "id": "bc134b13-5bcd-46db-b190-c15b0aa423f8",
   "metadata": {
    "language": "python",
    "name": "imports"
   },
   "outputs": [],
   "source": "import sklearn\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom snowflake.snowpark.context import get_active_session",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "09677676-7dee-430f-9dee-e4175ff55427",
   "metadata": {
    "language": "python",
    "name": "loading_data"
   },
   "outputs": [],
   "source": "session = get_active_session()\ndf = session.table('DW_RESEARCH.MLTABLES.TITANIC').to_pandas()\n\ndf.head(6) # top 6 records",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "299f7ee4-5a52-46f4-b288-b2c4165e38ee",
   "metadata": {
    "language": "python",
    "name": "looking_at_data"
   },
   "outputs": [],
   "source": "numSamples = len(df)\n\ndf.keys() # feature names - SURVIVED is a classification, won't work with linear regression without logistic regression. We will attempt to determine FARE\nnumFeatures = len(df.keys()) - 1 # one of the keys is the result\nfeatures = df.keys().drop(\"FARE\").to_list()\n\nprint(f\"\"\"num samples={numSamples}, num features={numFeatures}\"\"\")\nprint(f\"\"\"features={features}\"\"\")\n\nx = np.array(df[\"AGE\"])\ny = np.array(df[\"FARE\"])\nprint(\"The average fare was $%.2f.\" % np.mean(y))",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0ae44516-50a7-4c5d-b776-9cc83b2eedf8",
   "metadata": {
    "language": "python",
    "name": "checking_data"
   },
   "outputs": [],
   "source": "print(x)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "87fdd09b-cb48-4855-b8b0-11e76e0bafca",
   "metadata": {
    "name": "note_x_nan",
    "collapsed": false
   },
   "source": "you will notice that we null ages, we need to remove these. always a good idea to go through the data and do atleast a minimal amount of data cleaning."
  },
  {
   "cell_type": "code",
   "id": "328d5997-d90b-41d0-b33b-7ee4bb08a039",
   "metadata": {
    "language": "python",
    "name": "remove_nan"
   },
   "outputs": [],
   "source": "# getting rid of instances where age is nan and there is fare data\nmask = ~np.isnan(x)\nx = x[mask]\ny = y[mask]",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4d4c6edf-b23c-435f-b10d-a83dadefcbf0",
   "metadata": {
    "language": "python",
    "name": "visualizing_data"
   },
   "outputs": [],
   "source": "plt.scatter(x,y)\nplt.xlabel(\"Age (years)\")\nplt.ylabel(\"Fare ($)\")\nplt.grid(True)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "63c43116-5112-49df-a0a3-599923308b40",
   "metadata": {
    "name": "note_data",
    "collapsed": false
   },
   "source": "Notice that there is no distinct visual way to determine whether there is a trend between age and fare. Keep this in mind as we are looking at two possibly unrelated topics."
  },
  {
   "cell_type": "markdown",
   "id": "b51b7004-9d35-4235-b48f-baddbf83e94d",
   "metadata": {
    "name": "note_math1",
    "collapsed": false
   },
   "source": "### How are we determining the \"best\" regression for our data so far?\n\nResiduals. This is the difference between the actual data and our prediction.\\\n$$\\epsilon_{i}=y_{i}-ŷ_{i}$$\n\nWe want to find the model parameters that best minimize the residuals, one option is residual sum of squares (RSS).\n\n#### RSS - Residual Sum of Squares\nThis is often also called the sum of squared errors (SSE) or sum of square residuals (SSR).\\\n$$RSS\\left(\\beta_{0},\\beta_{1}\\right)=\\sum_{i=1}^{n}\\epsilon_{i}^{2}=\\sum_{i=1}^{n}\\left(y_{i}-ŷ_{i}\\right)^{2}=\\sum_{i=1}^{n}\\left(y_{i}-\\left(\\beta_{1}x_{i}+\\beta_{0}\\right)\\right)^{2}$$\\\nThe values of $$\\beta$$ that minimizes RSS is the least-squares fit.\n\n#### Where is this heading?\nThe general approach to a ML problem?\n* A model with some parameters\n* Get training data\n* Choose a loss function\n* Find parameters that minimize that loss (this is where we would use SGD to optimize loss)"
  },
  {
   "cell_type": "code",
   "id": "3e19a90b-55f3-4a5f-9e27-4eece0106c0d",
   "metadata": {
    "language": "python",
    "name": "fitting_linear_model"
   },
   "outputs": [],
   "source": "def fit_linear(x,y):\n    xm = np.mean(x) # feature mean\n    ym = np.mean(y) # target mean\n\n    sxx = np.mean((x-xm)**2) # sample variance\n    syy = np.mean((y-ym)**2)\n    sxy = np.mean((x-xm)*(y-ym))\n\n    beta1 = sxy/sxx\n    beta0 = ym - beta1*xm\n    rsq = sxy**2/sxx/syy\n\n    return beta0, beta1, rsq",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bc28e2d8-a981-45a6-a746-89b45b0b400a",
   "metadata": {
    "name": "note_math2",
    "collapsed": false
   },
   "source": "# How do we derive this?\n## Quick Statistics:\n### Sample mean:\n* $$\\overline{x}=\\frac{1}{n}\\sum_{i=1}^{n}x_{i}$$\n* $$\\overline{y}=\\frac{1}{n}\\sum_{i=1}^{n}y_{i}$$\n\n### Sample variance:\n* $$s_{x}^{2}=\\frac{1}{n}\\sum_{i=1}^{n}\\left(x_{i}-\\overline{x}\\right)^{2}$$\n* $$s_{y}^{2}=\\frac{1}{n}\\sum_{i=1}^{n}\\left(y_{i}-\\overline{y}\\right)^{2}$$\n\n***Sample standard deviation can be found by square-rooting the sample variance.***\n\n### Sample covariance:\n* $$s_{xy}=\\sum_{i=1}^{n}\\left(x_{i}-\\overline{x}\\right)\\left(y_{i}-\\overline{y}\\right)$$\n  + *this indicates how \"related\" $$x_{i}$$ and $$y_{i}$$ are*\n---\n## Minimizing the RSS:\nTo minimize the $$RSS\\left(\\beta_{0},\\beta_{1}\\right)$$, we find the $$\\beta_{0}$$ and $$\\beta_{1}$$ that zero the gradient.\n\nWhat is the gradient? You can think of it as the vector of partial derivatives. A partial derivative, somewhat similar to normal derivatives in calculus, is the rate of change of one variable while holding all other variables constant. \n\n$$\\left[\\frac{∂RSS\\left(\\beta_{0},\\beta_{1}\\right)}{∂\\beta_{0}},\\ \\frac{∂RSS\\left(\\beta_{0},\\beta_{1}\\right)}{∂\\beta_{1}}\\right]^{⊤}=\\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}$$ \n\n### Deriving $$\\beta_{0}$$:\n$$0=\\frac{∂RSS\\left(\\beta_{0},\\beta_{1}\\right)}{∂\\beta_{0}}=\\sum_{i}^{ }\\frac{∂}{∂\\beta_{0}}\\left(y_{i}-\\beta_{0}-\\beta_{1}x_{i}\\right)^{2}=\\sum_{i}^{ }-2\\left(y_{i}-\\beta_{0}-\\beta_{1}x_{i}\\right)$$\n\n$$0=\\sum_{i}^{ }-2\\left(y_{i}-\\beta_{0}-\\beta_{1}x_{i}\\right)$$ **→** $$0\\left(-\\frac{1}{2n}\\right)=-\\frac{1}{2n}\\left(\\sum_{i}^{ }-2\\left(y_{i}-\\beta_{0}-\\beta_{1}x_{i}\\right)\\right)$$ \n\n$$0=-\\frac{1}{2n}\\left(\\sum_{i}^{ }-2\\left(y_{i}-\\beta_{0}-\\beta_{1}x_{i}\\right)\\right)=\\frac{1}{n}\\sum_{i}^{n}\\left(y_{i}-\\beta_{0}-\\beta_{1}x_{i}\\right)=\\frac{1}{n}\\sum_{i=1}^{n}y_{i}-\\beta_{0}-\\beta_{1}\\frac{1}{n}\\sum_{i=1}^{n}x_{i}=\\overline{y}-\\beta_{0}-\\beta_{1}\\overline{x}$$\n\n$$0=\\overline{y}-\\beta_{0}-\\beta_{1}\\overline{x}$$ **↔** $$\\beta_{0}=\\overline{y}-\\beta_{1}\\overline{x}$$\\\n\n### Deriving $$\\beta_{1}$$:\n$$0=\\frac{∂RSS\\left(\\beta_{0},\\beta_{1}\\right)}{∂\\beta_{0}}=\\sum_{i}^{ }\\frac{∂}{∂\\beta_{1}}\\left(y_{i}-\\beta_{0}-\\beta_{1}x_{i}\\right)^{2}=\\sum_{i}^{ }-2\\left(y_{i}-\\beta_{0}-\\beta_{1}x_{i}\\right)x_{i}$$\n\n$$0=\\sum_{i}^{ }-2\\left(y_{i}-\\beta_{0}-\\beta_{1}x_{i}\\right)x_{i}$$ **→** $$0\\left(-\\frac{1}{2n}\\right)=-\\frac{1}{2n}\\left(\\sum_{i}^{ }-2\\left(y_{i}-\\beta_{0}-\\beta_{1}x_{i}\\right)x_{i}\\right)$$\n\n$$0=-\\frac{1}{2n}\\left(\\sum_{i=1}^{n}-2\\left(y_{i}-\\beta_{0}-\\beta_{1}x_{i}\\right)x_{i}\\right)=\\frac{1}{n}\\sum_{i=1}^{n}x_{i}\\left(y_{i}-\\beta_{0}-\\beta_{1}x_{i}\\right)=\\frac{1}{n}\\sum_{i=1}^{n}x_{i}y_{i}-\\frac{1}{n}\\sum_{i=1}^{n}x_{i}\\beta_{0}-\\beta_{1}\\frac{1}{n}\\sum_{i=1}^{n}x_{i}^{2}=\\frac{1}{n}\\sum_{i=1}^{n}x_{i}y_{i}-\\overline{x}\\beta_{0}-\\beta_{1}\\frac{1}{n}\\sum_{i=1}^{n}x_{i}^{2}$$\n\nSince we have already derived $$\\beta_{0}$$ in terms of $$\\beta_{1}$$ we can plug in our derivation in for $$\\beta_{0}$$ in \n\n$$0=\\frac{1}{n}\\sum_{i=1}^{n}x_{i}y_{i}-\\overline{x}\\beta_{0}-\\beta_{1}\\frac{1}{n}\\sum_{i=1}^{n}x_{i}^{2}$$\n\n$$0=\\frac{1}{n}\\sum_{i=1}^{n}x_{i}y_{i}-\\overline{x}\\beta_{0}-\\beta_{1}\\frac{1}{n}\\sum_{i=1}^{n}x_{i}^{2}=\\frac{1}{n}\\sum_{i=1}^{n}x_{i}y_{i}-\\overline{x}\\left(\\overline{y}-\\beta_{1}\\overline{x}\\right)-\\beta_{1}\\frac{1}{n}\\sum_{i=1}^{n}x_{i}^{2}=\\frac{1}{n}\\sum_{i=1}^{n}x_{i}y_{i}-\\overline{x}\\overline{y}-\\beta_{1}\\left(\\frac{1}{n}\\sum_{i=1}^{n}x_{i}^{2}-\\overline{x}^{2}\\right)=s_{xy}-\\beta_{1}\\left(s_{x}^{2}\\right)$$\n\n$$0=s_{xy}-\\beta_{1}\\left(s_{x}^{2}\\right)$$ **↔** $$\\beta_{1}=\\frac{s_{xy}}{s_{x}^{2}}$$\n\n---\n## Finding the minimum RSS\nTo find the minimum RSS, we can plug the optimal value of $$\\beta_{0}$$ and $$\\beta_{1}$$ into the RSS formula we had earlier:\n\n$$RSS\\left(\\beta_{0},\\beta_{1}\\right)=\\sum_{i}^{ }\\left(y_{i}-\\beta_{0}-\\beta_{1}x_{i}\\right)^{2}$$\n\n$$RSS\\left(\\beta_{0},\\beta_{1}\\right)=\\sum_{i}^{ }\\left(y_{i}-\\beta_{0}-\\beta_{1}x_{i}\\right)^{2}=\\sum_{i}^{ }\\left(\\left(y_{i}-\\overline{y}\\right)-\\beta_{1}\\left(x_{i}-\\overline{x}\\right)\\right)^{2}=\\sum_{i}^{ }\\left(y_{i}-\\overline{y}\\right)^{2}-2\\beta_{1}\\sum_{i}^{ }\\left(y_{i}-\\overline{y}\\right)\\left(x_{i}-\\overline{x}\\right)+\\beta_{1}^{2}\\sum_{i}^{ }\\left(x_{i}-\\overline{x}\\right)^{2}$$\n\nNow we plug in $$\\beta_{0}$$ and $$\\beta_{1}$$ to get:\n\n$$\\sum_{i}^{ }\\left(y_{i}-\\overline{y}\\right)^{2}-2\\beta_{1}\\sum_{i}^{ }\\left(y_{i}-\\overline{y}\\right)\\left(x_{i}-\\overline{x}\\right)+\\beta_{1}^{2}\\sum_{i}^{ }\\left(x_{i}-\\overline{x}\\right)^{2}=ns_{y}^{2}-2n\\frac{s_{xy}^{2}}{s_{x}^{2}}+n\\frac{s_{xy}^{2}}{s_{x}^{2}}=n\\left(s_{y}^{2}-2\\frac{s_{xy}^{2}}{s_{x}^{2}}+\\frac{s_{xy}^{2}}{s_{x}^{2}}\\right)=ns_{y}^{2}\\left(1-\\frac{s_{xy}^{2}}{s_{x}^{2}s_{y}^{2}}\\right)$$\n\nAnd a little fun-fact for us here is that **$$\\frac{s_{xy}}{s_{x}s_{y}}$$** is actually an important statistic called sample correlation coefficient denoted as $$\\rho$$.\n\n---\n## The takeaway\n$$\\beta_{0}=\\overline{y}-\\beta_{1}\\overline{x}$$\n\n$$\\beta_{1}=\\frac{s_{xy}}{s_{x}^{2}}$$\n\n$$RSS\\left(\\beta_{0},\\beta_{1}\\right)=ns_{y}^{2}\\left(1-\\frac{s_{xy}^{2}}{s_{x}^{2}s_{y}^{2}}\\right)$$ or we can denote it now as $$RSS\\left(\\beta_{0},\\beta_{1}\\right)=ns_{y}^{2}\\left(1-\\rho_{xy}^{2}\\right)$$"
  },
  {
   "cell_type": "code",
   "id": "e33a28e3-2679-4fa4-9e2c-1e8c1dd168fe",
   "metadata": {
    "language": "python",
    "name": "run_linear_model"
   },
   "outputs": [],
   "source": "beta0, beta1, rsq = fit_linear(x,y)\nprint(\"B0:\", \"%.2f\" % beta0, \"B1:\", \"%.2f\" % beta1, \"rsq:\", \"%.2f\" % rsq)\n\nplt.plot(x, beta1*x + beta0, color='red') # keep this in mind\n\nplt.axis('equal')\nplt.xlabel(\"Age (years)\")\nplt.ylabel(\"Predicted Fare ($)\")\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "559421e7-cb23-4bfe-99c2-dc9caa17568d",
   "metadata": {
    "name": "note_importance",
    "collapsed": false
   },
   "source": "### What are we plotting? \n\nWhat does beta1*x + beta0 look like?\n* The answer is y=mx+b, the y-int slope formula. \n\nWe are just determining the coefficients for the feature selected and its \"impact\" (in quotes because we need to do much more math to determine if its actually correlated or not)."
  },
  {
   "cell_type": "code",
   "id": "b20d885c-a9f6-438a-97f3-d784268e8abb",
   "metadata": {
    "language": "python",
    "name": "visualizing_result"
   },
   "outputs": [],
   "source": "# plotting the regression and the actual data on top of each other\nx_pred = np.arange(-50, 100)\nplt.plot(x_pred, beta1*x_pred + beta0, color='red')\nplt.scatter(x,y)\n\nplt.xlabel(\"Age (years)\")\nplt.ylabel(\"Fare ($)\")\nplt.xlim(-1, 80)\nplt.grid(True)\nplt.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4fd973e8-386f-4e19-9345-715092b6d3e2",
   "metadata": {
    "name": "note_takeaway",
    "collapsed": false
   },
   "source": "Just because the data looks like it doesn't fit to a linear regression, doesn't mean that linear regression isn't useful.\n\nYou can apply transformations to the regression itself, like 1/(mx+b)."
  }
 ]
}